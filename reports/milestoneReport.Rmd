---
title: "SwiftKey Training Data Major Features"
author: "Vasco Pereira"
date: "`r as.Date(Sys.time())`"
output: 
        html_document:
                toc: true
                toc_float: true
                toc_depth: 3
                theme: united
                highlight: breezedark
                df_print: paged
                code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.align = "center", message = FALSE, warning = FALSE)
#options(scipen = 999)
```

# Introduction {.tabset}

The objectives of this report is to:   

1. Inspect major features of the english data sets provided   
2. Create a plan to develop a:   

    2.1 Prediction algorithm   
    2.2 Shiny App 

## Summary

This report gives an insight of the raw data structure and summarises its characteristics (size, number of lines and number of words). For the principal focus of the prediction algorithm the `en_US.twitter.txt` was used. 
The data was cleaned, and explored by geting the one to four gram frequencies charts. Also it was made an interesting observation - a small set of more frequent words (36%) cover 90% of all terms in the corpora. By reducing the number of terms we may cover most of the instances with a smaller size data for a better user experience.


For more information please visit the repository of the present work at <https://github.com/VascoRibeiroPereira/dataScienceCapstone>

## Libraries

All libraries needed are here (check the code clicking the button).

```{r }

library(R.utils)
library(dplyr)
library(tm)
library(lexicon)
library(stringr)
library(tidyverse)
library(tidytext)
library(textclean)
library(tokenizers)
library(markovchain)
library(ngram)
library(ggplot2)
library(plotly)
library(wordcloud)
library(RColorBrewer)
library(textcat)

```

# Data Insight {.tabset}

The data was downloaded and unziped, but instead of open or read all data, only a small part was peeked in order to preserve the computer memory for other tasks.

```{r, cache=TRUE }

mywd <- dirname(getwd())
        
if (!file.exists("Coursera-SwiftKey.zip")) {
        download.file("https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",
                      "Coursera-SwiftKey.zip")
        unzip("Coursera-SwiftKey.zip")
}

file_insight <- function(fileName){
        
        queryFile <- paste(mywd, "/reports/final/en_US/", fileName, sep="")
        con <- file(queryFile, "r")
        preview <- readLines(con, 2, skipNul = TRUE)
        close(con)
        preview
}

```

There are three english files: `en_US.twitter.txt`, `en_US.blogs.txt` and `en_US.news.txt`.

Preview of `en_US.twitter.txt`:

```{r }

file_insight("en_US.twitter.txt")

```


Preview of `en_US.blogs.txt`:

```{r }

file_insight("en_US.blogs.txt")

```

Preview of `en_US.news.txt`:

```{r }

file_insight("en_US.news.txt")

```

The three files are composed of delimited text entries with paragraphs. Their dimentions are summarised in the next table:


```{r, cache=TRUE }

directory <- paste(mywd, "/reports/final/en_US/", sep="")
filesFull <- list.files(path = directory, full.names = T)
filenames <- lapply(directory, list.files)

filesSize_df <- tibble()
for (i in 1:length(filenames[[1]])) {

    tmp_size <- utils:::format.object_size(file.info(filesFull[i])[["size"]], "auto")
    tmp_name <- filenames[[1]][i]
    tmp_length <- length(readLines(filesFull[i], warn = FALSE))
    
    tmp_words <- count_words(readLines(filesFull[i], warn = FALSE))
    mean_words <- round(mean(tmp_words, na.rm = TRUE),0)
    min_words <- min(tmp_words, na.rm = TRUE)
    max_words <- max(tmp_words, na.rm = TRUE)
    

    tmp_row <- c(tmp_name, tmp_size, tmp_length, mean_words, min_words, max_words)
    
    filesSize_df <- rbind(filesSize_df, tmp_row, stringsAsFactors = FALSE)
}
names(filesSize_df) <- c("file names", "size", "lines", "mean words", "min words", "max words")

filesSize_df
```

This work will focus primarily in the `en_US.twitter.txt`. This data have `r filesSize_df[3,3]` entries, wich means that there's a lot of data to work with and may be impratical to import the complete file to perform data analysis due to memory limitations.

For the time being this work will be focused in ~5000 random selected entries of text from the `en_US.twitter.txt`, representing `r paste(round(5000/as.numeric(filesSize_df[3,3])*100,1), "%", sep="")` of the complete data.


# Cleaning the Data {.tabset}

The subset of the data was randomly selected with a `sample` function in relation to the choosen percentage of the number of entries. 

```{r, cache=TRUE }

subsetBigData <- function(file_Path, percentage) {
    file_length <- length(readLines(file_Path, warn = FALSE))
    number_Lines <- as.integer(file_length*(percentage/100))
    set.seed(123)
    lineSelection <- sample(file_length, number_Lines)
    con <- file(file_Path, "r")
    fileRead <- readLines(con, skipNul = TRUE)[lineSelection]
    close(con)
    return(fileRead)
}

enDataSubset <- subsetBigData(paste(directory, "/en_US.twitter.txt", sep=""), .2)

```

For cleaning purposes the data was transformed in a `Volatile Corpora` with the **`tm`** package.

```{r, cache=TRUE }
corp_toClean <- VCorpus(VectorSource(enDataSubset))
```

A function named `clean_corpus` was made in order to clean the *Corpora* with this features:   

1. Replacing contractions *(i.e. "didn't" = "did not")*   
2. Remove punctuation *(i.e. "?" = "")*   
3. Replace money *(i.e. "$3" = "three dollars")*   
4. Replace emoticon *(i.e. ":)" = "smiley")*   
5. Replace symbol *(i.e. "i am @ home" = "i am  at  home")*   
6. Replace word elongation *(i.e. "sooo goooood" = "so good")*   
7. Replace ordinal *(i.e. "1st" = "first")*   
8. Replace slang *(i.e. "RT" = "Retweet")*   
9. Replace number *(i.e. "1" = "one")*   
10. Replace emoji *(i.e. "U0001f60d" = "smiling face with heart-eyes")*   
11. Remove white space *(i.e. "Hello     World" = "Hello World")*   
12. Replace to lower case *(i.e. "Ham" = "ham")*   
13. Remove profane words    

``` {r }

## Expanding the contractions lexicon
new_con <- data.frame("here's", "here is") 
names(new_con) <- c("contraction", "expanded")
key_contractions <- rbind(key_contractions, new_con)

## Expanding Slang df
new_slang <- data.frame("RT", "Retweet") 
names(new_slang) <- c("x", "y")
hash_internet_slang <- rbind(hash_internet_slang, new_slang)

## Cleaning and coercing all profanity data sources
alvarez_alternative <- str_replace_all(profanity_alvarez, "\\*", "\\\\*")
alvarez_alternative <- str_replace_all(alvarez_alternative, "\\(", "\\\\(")
anger_alternative <- str_replace_all(profanity_zac_anger, "\\*", "\\\\*")
anger_alternative <- str_replace_all(anger_alternative, "\\(", "\\\\(")
profaneWords <- unique(tolower(c(profanity_arr_bad, 
                                 profanity_banned,
                                 profanity_racist,
                                 alvarez_alternative,
                                 anger_alternative)))

## Function
clean_corpus <- function(corpus){
        
        for (i in 1:length(corpus)) corpus[[i]] <- corpus[[i]] %>%
                        replace_contraction(contraction.key = key_contractions) %>%
                        removePunctuation(preserve_intra_word_contractions = TRUE,
                                          preserve_intra_word_dashes = TRUE,
                                          ucp = TRUE) %>%
                        replace_money() %>%
                        replace_emoticon() %>%
                        replace_symbol() %>%
                        replace_word_elongation() %>%
                        replace_ordinal() %>%
                        replace_internet_slang(slang = paste0("\\b",
                                                              hash_internet_slang[[1]], "\\b"),
                                               replacement = hash_internet_slang[[2]], ignore.case = TRUE) %>%
                        replace_number() %>%
                        replace_emoji() %>%
                        replace_non_ascii() %>%
                        stripWhitespace() %>%
                        tolower() %>%
                        removeWords(profaneWords)        
        return(corpus)
}

```


```{r, cache=TRUE }

corp <- clean_corpus(corp_toClean)

```


An example of the text cleaning in action:

- Before

```{r }

corp_toClean[[2]][1]$content

```   

- After   

```{r }

corp[[2]][1]

```

# Data Exploration {.tabset}

Although the `Corpus` class is nice to apply cleaning methods by the `tm` and `textclean` packages, and also to preserve metadata, it is not very usefull for data visualization and for modeling purposes. The data was transformed into a `character string`, with which it will be worked from now on.

```{r, cache=TRUE }

## Corpus to string
corp_str <- as.character(unlist(corp))
## Remove NA introduced by the cleaning algorithm
corp_str <- corp_str[!is.na(corp_str)]
## Preprocess

corp_final <- as.character()

for (i in 1:length(corp_str)){
    
    tmp <- preprocess(corp_str[i], case = "lower", 
                       remove.punct = TRUE, 
                       remove.numbers = TRUE, 
                       fix.spacing = FALSE)
    
    corp_final <- c(corp_final, tmp)
    
}

```

In order to ilustrate important features of the data it is usefull to create ngrams of the words. The intention is to understand the frequency of each term alone (unigram) or in chunks of 2 (bigram), 3 (trigram) or 4 (tetragram) words.

```{r }

## Function that creates the n-grams. This function accounts for the number of words in each setence 
## x is a character string
## n is the 'n' as in 'n-gram'

ngram_freq <- function(x, n) {
        if (n > 1) {
        n_gram <- as_tibble(get.phrasetable(ngram(x[count_words(x) > n], n)))
        }else{
        n_gram <- as_tibble(get.phrasetable(ngram(x, 1)))
        }
        n_gram$ngrams <- gsub(" $", "", n_gram$ngrams)
        return(n_gram)
}

```

## nGrams

### Unigrams

Graphical representation of the unigram frequency wih the most frequent unigrams (>=300 occurrences) and its respective wordcloud with the 50 most frequent words.

```{r, cache=TRUE }

ng_1 <- ngram_freq(corp_final, 1)

plot_1_gram <- ggplot(subset(ng_1, freq %in% 300:as.integer(ng_1[1,2])), aes(x = reorder(ngrams, freq), freq)) + 
        geom_col(aes(fill = freq)) + 
        xlab("Terms") +
        ylab("Frequency") +
        ggtitle("Unigram Frequency Chart", subtitle = waiver()) +
        scale_fill_gradient(low = "green", 
                            high = "red") +
        coord_flip()
ggplotly(plot_1_gram)

```

```{r, cache=TRUE }

set.seed(1500)
wordcloud(ng_1$ngrams[1:50], ng_1$freq[1:50], colors=brewer.pal(max(12,ncol(ng_1)),"Paired"))

```


### Bigrams

Graphical representation of the bigram frequency wih the most frequent unigrams (>=80 occurrences) and its respective wordcloud with the 50 most frequent words.

```{r, cache=TRUE }

ng_2 <- ngram_freq(corp_final, 2)

plot_2_gram <- ggplot(subset(ng_2, freq %in% 80:as.integer(ng_2[1,2])), aes(x = reorder(ngrams, freq), freq)) + 
        geom_col(aes(fill = freq)) + 
        xlab("Terms") +
        ylab("Frequency") +
        ggtitle("Bigram Frequency Chart", subtitle = waiver()) +
        scale_fill_gradient(low = "green", 
                            high = "red") +
        coord_flip()
ggplotly(plot_2_gram)

```

```{r, cache=TRUE }

set.seed(1500)
wordcloud(ng_2$ngrams[1:50], ng_2$freq[1:50], colors=brewer.pal(max(12,ncol(ng_2)),"Paired"))

```


### Trigrams

Graphical representation of the trigram frequency wih the most frequent unigrams (>=15 occurrences) and its respective wordcloud with the 40 most frequent words.

```{r, cache=TRUE }

ng_3 <- ngram_freq(corp_final, 3)

plot_3_gram <- ggplot(subset(ng_3, freq %in% 15:as.integer(ng_3[1,2])), aes(x = reorder(ngrams, freq), freq)) + 
        geom_col(aes(fill = freq)) + 
        xlab("Terms") +
        ylab("Frequency") +
        ggtitle("Trigram Frequency Chart", subtitle = waiver()) +
        scale_fill_gradient(low = "green", 
                            high = "red") +
        coord_flip()
ggplotly(plot_3_gram)

```

```{r, cache=TRUE }

set.seed(1200)
wordcloud(ng_3$ngrams[1:40], ng_3$freq[1:40], colors=brewer.pal(max(12,ncol(ng_3)),"Paired"))

```

### Tetragrams

Graphical representation of the tetragram frequency wih the most frequent unigrams (>=5 occurrences) and its respective wordcloud with the 40 most frequent words.

```{r, cache=TRUE }

ng_4 <- ngram_freq(corp_final, 4)


plot_4_gram <- ggplot(subset(ng_4, freq %in% 5:as.integer(ng_4[1,2])), aes(x = reorder(ngrams, freq), freq)) + 
        geom_col(aes(fill = freq)) + 
        xlab("Terms") +
        ylab("Frequency") +
        ggtitle("Tetragram Frequency Chart", subtitle = waiver()) +
        scale_fill_gradient(low = "green", 
                            high = "red") +
        coord_flip()
ggplotly(plot_4_gram)

```

```{r, cache=TRUE }

set.seed(1200)
wordcloud(ng_4$ngrams[1:40], ng_4$freq[1:40], colors=brewer.pal(max(12,ncol(ng_4)),"Paired"))

```


### Unique Words

About the question of how many unique words would cover most of our word instances.

```{r, cache=TRUE }

c_ng1 <- ng_1 %>%
        mutate(Cumulative = cumsum(prop))

plot_c_ng1 <- ggplot(c_ng1, aes(1:dim(c_ng1)[1], Cumulative)) + 
        geom_line() +
        annotate("rect", xmin = 0, xmax = dim(c_ng1 %>% filter(Cumulative <= .500))[1], 
                 ymin = 0, ymax = .50, alpha = .4) +
        annotate("text", x = 1000, y = .25, label = "<50% Cover") +
        annotate("rect", xmin = 0, xmax = dim(c_ng1 %>% filter(Cumulative <= .900))[1], 
                 ymin = 0, ymax = .90, alpha = .3) +
        annotate("text", x = 2500, y = .75, label = "90% Cover") +
        annotate("rect", xmin = 0, xmax = dim(c_ng1 %>% filter(Cumulative <= 1.000))[1], 
                 ymin = 0, ymax = 1.00, alpha = .2) +
        annotate("text", x = 8500, y = .90, label = "100% Cover") +
        xlab("Number of Terms") +
        ylab("Cumulative Proportion")

ggplotly(plot_c_ng1)

```

The graph of the **number of terms** versus **cumulative proportion** reveals that only `r dim(filter(c_ng1, Cumulative <= .500))[1]` terms are needed to cover 50% of words instances and `r dim(filter(c_ng1, Cumulative <= .900))[1]` for 90%, in a universe of `r dim(c_ng1)[1]` terms.

### nGram Observations

```{r, cache=TRUE }

stopSubset <- as.numeric()
uniqueStopWords <- unique(stop_words$word)

for (i in 1:length(stop_words$word)){
    
    loc_tmp <- grep(paste("^",uniqueStopWords[i],"$", sep=""), ng_1$ngrams)
    stopSubset <- c(stopSubset, loc_tmp)
    
}

freqStopWords <- sum((ng_1[stopSubset,])$freq)

```

As it was expected the most frequent words are consistent with the `stop_words` data frame from the package `tidytext`.

By comparing the words from our sample with the `stop_words` data frame we observe `r length(stopSubset)` unique stop words in a universe of `r length(ng_1$ngrams)` words. Our stop word percentage without accounting for the frequency is `r round(length(stopSubset)/length(ng_1$ngrams) *100,1)`%.

The impact of this words is remarkable, with a little percentage of unique words, having the frequency into account we get `r round(freqStopWords/sum(ng_1$freq) *100, 1)`%!

The current work must take the stop words into account since the intention is to predict text (independent of its nature), although it would be necessary to remove the stop words in many other contexts (i.e. sentiment analysis).

For the prediction model, it can que usefull to persuit a reduction of size and runtime working only with the 90% of the ngrams.


## Language

It is interesting to understand the presence of foreign languages in the text in order to understand if it impacts significantly the prediction algorithm (intended to be in english in the current work). For that the `textcat` package was used with the profile `ECIMCI_profiles` and the method `Dice`.

```{r, cache=TRUE }

lang_ng1 <- textcat(ng_1$ngrams, p=textcat::ECIMCI_profiles, method = "Dice")
lang_ng1_df <- as_tibble(table(lang_ng1)) %>% arrange(desc(n))
lang_ng1_df <- lang_ng1_df %>% mutate(ngram = "Unigram") %>%
        rename(language = lang_ng1)
 

lang_ng2 <- textcat(ng_2$ngrams, p=textcat::ECIMCI_profiles, method = "Dice")
lang_ng2_df <- as_tibble(table(lang_ng2)) %>% arrange(desc(n))
lang_ng2_df <- lang_ng2_df %>% mutate(ngram = "Bigram") %>%
        rename(language = lang_ng2)

lang_ng3 <- textcat(ng_3$ngrams, p=textcat::ECIMCI_profiles, method = "Dice")
lang_ng3_df <- as_tibble(table(lang_ng3)) %>% arrange(desc(n))
lang_ng3_df <- lang_ng3_df %>% mutate(ngram = "Trigram") %>%
        rename(language = lang_ng3)


# The language identification inproves whith the for each n+1 gram

lang_ng_n <- bind_rows(lang_ng1_df, lang_ng2_df, lang_ng3_df)
langPlot <- ggplot(lang_ng_n, aes(language, n)) + geom_col() # barplot
neworder <- c("Unigram","Bigram","Trigram")
langPlot <- langPlot + facet_wrap( ~ factor(ngram,levels=neworder), nrow = 3) + theme(legend.position = "none")

ggplotly(langPlot)

```

The language identification varies for english in this fashion: Unigram < Bigram < Trigram. It is a fact that most words in the provided text are english but some terms may be confused with other languages and the context (added with the multi-grams) narrows down the error in the identifications. Assuming the Trigrams as the less error prone we have `r round((lang_ng3_df %>% filter(language == "en"))$n/sum(lang_ng3_df$n)*100)`% of words in english.


## Parts of Speech (POS)

The POS - Parts pf Speech, can include an interesting feature to the future algorithm, since it can add a smart and eficient way to understand the next most probable word in the context. 
In an over simplification, "hello world" is labeled as "INTJ NOUN", as such the algorithm would try to find a NOUN to an INTJ - *for example user: "hello", suggested: "vasco"*.
The package `RDRPOSTagger` can be helpfull in this subject and its usefullness will be accessed during the prediction model development.

# Prediction Algorithm {.tabset}

The current plan for the prediction algorithm is:   

- Check feasibility to include:   
    - Language determination
    - POS
    - Only `r round((dim(filter(c_ng1, Cumulative <= .900))[1]/dim(c_ng1)[1])*100)`% of terms (corresponding to 90% coverage)

- Fit the n-grams model with Markov Chains:   
    - Using all the n-grams by combination (solving unobserved cases but adding less accurate results)   
    - Explore the possibility of include Topic Modeling    
        - Latent Dirichlet Allocation (LDA)    
            - LDA assume that the document is a bag of words, the documents should be big (not the twitter case)   


